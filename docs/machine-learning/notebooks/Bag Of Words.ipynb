{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "treebank_wordnet_pos = {\n",
    "    'J': 'a', # adjective\n",
    "    'V': 'v', # verb\n",
    "    'N': 'n', # noun\n",
    "    'R': 'r', # adverb\n",
    "}\n",
    "def get_wordnet_pos(treebank_pos, default='n'):\n",
    "    return treebank_wordnet_pos.get(treebank_pos[0], default)\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt    = re.sub(r'\\W+', ' ', txt).lower()\n",
    "    tokens = word_tokenize(txt)\n",
    "\n",
    "    return [\n",
    "        normalizer.lemmatize(token[0], get_wordnet_pos(token[1]))\n",
    "        for token in pos_tag(tokens)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW dictionary\n",
    "\n",
    "One of the most common ways to implement the BoW model in Python is as a dictionary where each word appearing in the document (key) is associated with the number of times it appears (value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'love': 1,\n",
       " 'fantastic': 2,\n",
       " 'fly': 2,\n",
       " 'fish': 3,\n",
       " 'these': 1,\n",
       " 'be': 1,\n",
       " 'just': 1,\n",
       " 'ok': 1,\n",
       " 'so': 1,\n",
       " 'maybe': 1,\n",
       " 'will': 1,\n",
       " 'find': 1,\n",
       " 'another': 1,\n",
       " 'few': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_bow(txt):\n",
    "    bow_dictionary = {}\n",
    "    tokens = preprocess_text(txt)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in bow_dictionary:\n",
    "            bow_dictionary[token] += 1\n",
    "        else:\n",
    "            bow_dictionary[token] = 1\n",
    "\n",
    "    return bow_dictionary\n",
    "\n",
    "txt = \"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"\n",
    "text_to_bow(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW vector\n",
    "\n",
    "Sometimes a dictionary just won’t fit the bill. Topic modelling applications, for example, require an implementation of bag-of-words that is a bit more mathematical: feature vectors. Turning text into a BoW vector is known as feature extraction or vectorization.\n",
    "\n",
    "When building BoW vectors, we generally\n",
    "1. Create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices. In other words, assign an index to each word in the corpus.<br><br>\n",
    "\n",
    "2. Using this dictionary, convert new documents into vectors using a vectorization function: create a vector of 0s, with a length of all known words, and count how many times each word appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'five': 0,\n",
       " 'fantastic': 1,\n",
       " 'fish': 2,\n",
       " 'fly': 3,\n",
       " 'off': 4,\n",
       " 'to': 5,\n",
       " 'find': 6,\n",
       " 'faraway': 7,\n",
       " 'function': 8,\n",
       " 'maybe': 9,\n",
       " 'another': 10,\n",
       " 'my': 11,\n",
       " 'with': 12,\n",
       " 'a': 13,\n",
       " 'please': 14}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_features_dictionary(documents):\n",
    "    features_dictionary = {}\n",
    "    tokens = preprocess_text(\" \".join(documents))\n",
    "\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if token not in features_dictionary:\n",
    "            features_dictionary[token] = i\n",
    "            i += 1\n",
    "    return features_dictionary\n",
    "\n",
    "training_documents = [\n",
    "    \"Five fantastic fish flew off to find faraway functions.\",\n",
    "    \"Maybe find another five fantastic fish?\",\n",
    "    \"Find my fish with a function please!\"\n",
    "]\n",
    "features_dictionary = create_features_dictionary(training_documents)\n",
    "features_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_bow_vector(txt, features_dictionary):\n",
    "    bow_vector = len(features_dictionary)*[0]\n",
    "    tokens = preprocess_text(txt)\n",
    "\n",
    "    for token in tokens:\n",
    "        i = features_dictionary[token]\n",
    "        bow_vector[i] += 1\n",
    "\n",
    "    return bow_vector\n",
    "\n",
    "txt = \"Another five fish find another faraway fish.\"\n",
    "\n",
    "text_to_bow_vector(txt, features_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn\n",
    "\n",
    "For text_to_bow(), we can approximate the functionality with the collections module’s Counter() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'i': 2,\n",
       "         'love': 1,\n",
       "         'fantastic': 2,\n",
       "         'fly': 2,\n",
       "         'fish': 3,\n",
       "         'these': 1,\n",
       "         'be': 1,\n",
       "         'just': 1,\n",
       "         'ok': 1,\n",
       "         'so': 1,\n",
       "         'maybe': 1,\n",
       "         'will': 1,\n",
       "         'find': 1,\n",
       "         'another': 1,\n",
       "         'few': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "txt = \"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"\n",
    "tokens = preprocess_text(txt)\n",
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vectorization, we can use [CountVectorizer](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L808) from the machine learning library scikit-learn. Use fit() to train the features dictionary and then transform() to transform text into a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7f54e6e4f400>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __call__(self, txt):\n",
    "        return preprocess_text(txt)\n",
    "\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    tokenizer=LemmaTokenizer()\n",
    ")\n",
    "bow_vectorizer.fit(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'five': 6,\n",
       " 'fantastic': 2,\n",
       " 'fish': 5,\n",
       " 'fly': 7,\n",
       " 'off': 11,\n",
       " 'to': 13,\n",
       " 'find': 4,\n",
       " 'faraway': 3,\n",
       " 'function': 8,\n",
       " 'maybe': 9,\n",
       " 'another': 1,\n",
       " 'my': 10,\n",
       " 'with': 14,\n",
       " 'a': 0,\n",
       " 'please': 12}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'another',\n",
       " 'fantastic',\n",
       " 'faraway',\n",
       " 'find',\n",
       " 'fish',\n",
       " 'five',\n",
       " 'fly',\n",
       " 'function',\n",
       " 'maybe',\n",
       " 'my',\n",
       " 'off',\n",
       " 'please',\n",
       " 'to',\n",
       " 'with']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that words in sklearn's feature dictionary are sorted in alphabetical order — so the bow vectorizer here won't be in the same order as ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 1 1 2 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "txt = \"Another five fish find another faraway fish.\"\n",
    "\n",
    "bow_vector = bow_vectorizer.transform([txt])\n",
    "print(bow_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW also has several advantages over other language models.\n",
    "\n",
    "1. It’s an easier model to get started with and a few Python libraries already have built-in support for it.\n",
    "\n",
    "2. Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity (i.e., it has more training knowledge to draw from)\n",
    "\n",
    "3. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.\n",
    "\n",
    "Alas, there is a trade-off for all the brilliance BoW brings to the table.\n",
    "\n",
    "1. Unless you want sentences that look like “the a but for the”, BoW is NOT a great primary model for text prediction — the probability of the following word is always just the most frequently used words.\n",
    "\n",
    "2. The BoW model’s word tokens lack context, which can make a word’s intended meaning unclear. Ex: if you look at the original text you may find that in fact every “good” was preceded by a “not.”\n",
    "\n",
    "3. Like all statistical models, BoW suffers from overfitting when it comes to vocabulary. What happens if the model comes across a new word that wasn’t in the training data?\n",
    "\n",
    "## Spam Classifier\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/SMSSpamCollection.tsv',\n",
    "    delimiter='\\t',\n",
    "    header=None,\n",
    "    names=['category','text']\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4179"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['category'], random_state=0\n",
    ")\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __call__(self, txt):\n",
    "        return preprocess_text(txt)\n",
    "\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    tokenizer=LemmaTokenizer()\n",
    ")\n",
    "training_vectors = bow_vectorizer.fit_transform(X_train)\n",
    "test_vectors     = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '00', '000', '000pes', '008704050406']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions for the test data were 98.78% accurate.\n",
      "f1-score: 99.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def spam_or_not(label): return \"spam\" if label else \"ham\"\n",
    "\n",
    "y_predict = spam_classifier.predict(test_vectors)\n",
    "accuracy  = accuracy_score(y_test, y_predict)\n",
    "f1score   = f1_score(y_test, y_predict, pos_label='ham')\n",
    "\n",
    "print(\"The predictions for the test data were {:.2f}% accurate.\"\n",
    "      .format(accuracy * 100))\n",
    "\n",
    "print(\"f1-score: {:.2f}%\"\n",
    "      .format(f1score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, 'I am not having her number sir' was classified as spam (wrongfully).\n",
      "\n",
      "Meanwhile, 'Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@' was classified as spam (rightfully).\n"
     ]
    }
   ],
   "source": [
    "errors = y_test[y_predict != y_test].index\n",
    "idx    = errors[0]\n",
    "\n",
    "print(\"For example, '{:s}' was classified as {:s} (wrongfully).\"\n",
    "      .format(X_test.iloc[idx], spam_or_not(y_predict[idx]))\n",
    ")\n",
    "print(\"\\nMeanwhile, '{:s}' was classified as {:s} (rightfully).\"\n",
    "      .format(X_test.iloc[0], spam_or_not(y_predict[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
